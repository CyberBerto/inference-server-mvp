# vLLM + LMCache Backend Container
# For RunPod deployment, use their official vLLM template instead.
#
# This Dockerfile is for local development or custom deployments.
# Requires NVIDIA GPU with CUDA 12.x

FROM nvidia/cuda:12.4.0-devel-ubuntu22.04

# Set environment
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CUDA_HOME=/usr/local/cuda

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.12 \
    python3.12-venv \
    python3-pip \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create venv and install dependencies
RUN python3.12 -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Install PyTorch with CUDA support
RUN pip install --no-cache-dir \
    torch==2.7.1 --index-url https://download.pytorch.org/whl/cu124

# Install vLLM and LMCache
RUN pip install --no-cache-dir \
    vllm==0.11.0 \
    lmcache==0.3.9

# Copy configuration
COPY configs/lmcache.yaml /app/configs/

# Environment for LMCache
ENV LMCACHE_CONFIG_FILE=/app/configs/lmcache.yaml

# Default model (override with --build-arg or runtime env)
ARG MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
ENV MODEL_NAME=${MODEL_NAME}

# Expose vLLM port
EXPOSE 8080

# Run vLLM server
CMD ["sh", "-c", "vllm serve ${MODEL_NAME} \
    --port 8080 \
    --max-model-len 131072 \
    --gpu-memory-utilization 0.9 \
    --max-num-seqs 64 \
    --enable-chunked-prefill \
    --kv-transfer-config '{\"kv_connector\":\"LMCacheConnectorV1\",\"kv_role\":\"kv_both\"}'"]

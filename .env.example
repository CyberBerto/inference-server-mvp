# Inference Server MVP Configuration
# Copy to .env and fill in values

# ============ Server ============
HOST=0.0.0.0
PORT=8000
DEBUG=false

# ============ vLLM Backend ============
VLLM_BASE_URL=http://localhost:8080
REQUEST_TIMEOUT=300.0

# ============ Model Metadata (for OpenRouter) ============
MODEL_ID=your-org/your-model
MODEL_DISPLAY_NAME=Your Model Display Name
ORGANIZATION_ID=your-org
MAX_CONTEXT_LENGTH=131072
QUANTIZATION=fp16

# ============ Pricing (per token, USD string) ============
PRICE_PER_PROMPT_TOKEN=0.000008
PRICE_PER_COMPLETION_TOKEN=0.000024

# ============ Hugging Face (for model download) ============
HUGGING_FACE_HUB_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx

# ============ LMCache ============
LMCACHE_ENABLED=true
LMCACHE_CONFIG_PATH=configs/lmcache.yaml
